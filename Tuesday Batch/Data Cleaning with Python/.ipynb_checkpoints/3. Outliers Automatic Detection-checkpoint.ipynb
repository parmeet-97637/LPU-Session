{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e40528",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "### Table of contens\n",
    "    \n",
    "* [1. Outlier Detection and Removal](#Topic-1)\n",
    "* [2. Dataset and Performance Baseline](#Topic-2)\n",
    "    * [a. House Price Regression Dataset](#Section-2.1)\n",
    "    * [b. Baseline Model Performance](#Section-2.2)\n",
    "* [3. Automatic Outlier Detection](#Topic-3)\n",
    "    * [a. Isolation Forest](#Section-3.1)\n",
    "    * [b. Minimum Covariance Determinant](#Section-3.2)\n",
    "    * [c. Local Outlier Factor](#Section-3.3)\n",
    "    * [d. One-Class SVM](#Section-3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9bc69",
   "metadata": {},
   "source": [
    "## 1. Outlier Detection and Removal <a class=\"Outlier Detection and Removal\" id=\"Topic-1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13969c8",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Outlier is the observations that are far from the rest of the observations or the center of mass of observations.\n",
    "\n",
    "This is easy to understand when we have one or two variables and we can visualize the data as a histogram or scatter plot, although it becomes very challenging when we have many input variables defining a high-dimensional input feature space.\n",
    "\n",
    "Simple statistical methods for identifying outliers can break down, such as methods that use standard deviations or the interquartile range.\n",
    "\n",
    "Outliers can skew statistical measures and data distributions, providing a misleading representation of the underlying data and relationships. Removing outliers from training data prior to modeling can result in a better fit of the data and, in turn, more skillful predictions.\n",
    "\n",
    "There are a variety of automatic model-based methods for identifying outliers in input data. Importantly, each method approaches the definition of an outlier is slightly different ways, providing alternate approaches to preparing a training dataset that can be evaluated and compared, just like any other data preparation step in a modeling pipeline.\n",
    "\n",
    "Before we dive into automatic outlier detection methods, let’s first select a standard machine learning dataset that we can use as the basis for our investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026561bb",
   "metadata": {},
   "source": [
    "## 2. Dataset and Performance Baseline <a class =\"Dataset and Performance Baseline\" id =\"Topic-2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12f5b3",
   "metadata": {},
   "source": [
    "<font size=\"3\">In this section, we will first select a standard machine learning dataset and establish a baseline in performance on this dataset.\n",
    "\n",
    "This will provide the context for exploring the outlier identification and removal method of data preparation in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827bc35",
   "metadata": {},
   "source": [
    "### 2.a. House Price Regression Dataset <a class =\"House Price Regression Dataset\" id =\"Section-2.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b0d05",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=\"3\">\n",
    "    \n",
    "We will use the house price regression dataset.\n",
    "\n",
    "This dataset has 13 input variables that describe the properties of the house and suburb and requires the prediction of the median value of houses in the suburb in thousands of dollars.\n",
    "\n",
    "You can learn more about the dataset here:\n",
    "- <a href =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv\"> Housing Price Dataset(housing.csv)</a>\n",
    "- <a href =\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.names\"> Housing Price Dataset Description(housing.names)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ed39bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b566ce",
   "metadata": {},
   "source": [
    "### 2.b Baseline Model Performance<a class =\"Baseline Model Performance\" id =\"Section-2.2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e4942",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "It is a regression predictive modeling problem, meaning that we will be predicting a numeric value. All input variables are also numeric.\n",
    "\n",
    "In this case, we will fit a linear regression algorithm and evaluate model performance by training the model on the test dataset and making a prediction on the test data and evaluate the predictions using the mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2803698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33654aa",
   "metadata": {},
   "source": [
    "<font size=\"3\">This provides a baseline in performance to which we can compare different outlier identification and removal procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35bf3d",
   "metadata": {},
   "source": [
    "## 3. Automatic Outlier Detection <a class=\"Automatic Outlier Detection\" id=\"Topic-3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7b668",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The scikit-learn library provides a number of built-in automatic methods for identifying outliers in data.\n",
    "\n",
    "In this section, we will review four methods and compare their performance on the house price dataset.\n",
    "\n",
    "Each method will be defined, then fit on the training dataset. The fit model will then predict which examples in the training dataset are outliers and which are not (so-called inliers). The outliers will then be removed from the training dataset, then the model will be fit on the remaining examples and evaluated on the entire test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d75e3",
   "metadata": {},
   "source": [
    "### 3.a Isolation Forest <a class=\"Isolation Forest\" id =\"Section-3.1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff48fbd",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Isolation Forest, or iForest for short, is a tree-based anomaly detection algorithm.\n",
    "\n",
    "It is based on modeling the normal data in such a way as to isolate anomalies that are both few in number and different in the feature space.\n",
    "    \n",
    "Perhaps the most important hyperparameter in the model is the “contamination” argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d0fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d1ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31735c",
   "metadata": {},
   "source": [
    "<font size=\"3\">Once identified, we can remove the outliers from the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e468092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b272ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305, 13) (305,)\n",
      "MAE: 3.263\n"
     ]
    }
   ],
   "source": [
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dfd875",
   "metadata": {},
   "source": [
    "### 3.b Minimum Covariance Determinant<a class=\"Minimum Covariance Determinant\" id =\"Section-3.2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed8f02",
   "metadata": {},
   "source": [
    "<font size =\"3\">\n",
    "\n",
    "If the input variables have a Gaussian distribution, then simple statistical methods can be used to detect outliers.\n",
    "\n",
    "For example, if the dataset has two input variables and both are Gaussian, then the feature space forms a multi-dimensional Gaussian and knowledge of this distribution can be used to identify values far from the distribution.\n",
    "\n",
    "This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. An efficient implementation of this technique for multivariate data is known as the Minimum Covariance Determinant, or MCD for short.\n",
    "    \n",
    "The scikit-learn library provides access to this method via the *EllipticEnvelope class*.\n",
    "\n",
    "It provides the “contamination” argument that defines the expected ratio of outliers to be observed in practice. In this case, we will set it to a value of 0.01, found with a little trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a27f4e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(335, 13) (335,)\n",
      "MAE: 3.388\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using elliptical envelope\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f4c6b",
   "metadata": {},
   "source": [
    "<font size=\"3\">In this case, we can see that the elliptical envelope method identified and removed only 4 outliers, resulting in a drop in MAE from 3.417 with the baseline to 3.388"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce5914",
   "metadata": {},
   "source": [
    "### 3.c Local Outlier Factor<a class=\"Local Outlier Factor\" id =\"Section-3.3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bf921",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space.\n",
    "\n",
    "This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality.\n",
    "\n",
    "The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers.\n",
    "\n",
    "The scikit-learn library provides an implementation of this approach in the LocalOutlierFactor class.\n",
    "\n",
    "The model provides the “contamination” argument, that is the expected percentage of outliers in the dataset, be indicated and defaults to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ef69e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(305, 13) (305,)\n",
      "MAE: 3.356\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using local outlier factor\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b088ecc",
   "metadata": {},
   "source": [
    "<font size=\"3\">In this case, we can see that the local outlier factor method identified and removed 34 outliers, the same number as isolation forest, resulting in a drop in MAE from 3.417 with the baseline to 3.356. Better, but not as good as isolation forest, suggesting a different set of outliers were identified and removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39921449",
   "metadata": {},
   "source": [
    "### 3.d One-Class SVM<a class=\" One-Class SVMr\" id =\"Section-3.4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26b765",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "The support vector machine, or SVM, algorithm developed initially for binary classification can be used for one-class classification.\n",
    "\n",
    "When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as One-Class SVM.\n",
    "\n",
    "The scikit-learn library provides an implementation of one-class SVM in the OneClassSVM class.\n",
    "\n",
    "The class provides the “nu” argument that specifies the approximate ratio of outliers in the dataset, which defaults to 0.1. In this case, we will set it to 0.01, found with a little trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ee5cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(336, 13) (336,)\n",
      "MAE: 3.431\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using one class SVM\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "ee = OneClassSVM(nu=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b46029",
   "metadata": {},
   "source": [
    "<font size=\"3\">n this case, we can see that only three outliers were identified and removed and the model achieved a MAE of about 3.431, which is not better than the baseline model that achieved 3.417. Perhaps better performance can be achieved with more tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8353569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
